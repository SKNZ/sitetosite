\documentclass[paper=a4, fontsize=11pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{fourier} % Adobe Utopia
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{relsize}
\usepackage{svg}

\usepackage{sectsty}
\allsectionsfont{\normalfont\scshape} 

\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}
\fancyfoot[L]{} 
\fancyfoot[C]{}
\fancyfoot[R]{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\setlength{\headheight}{14.6pt}

\usepackage{stmaryrd}
\usepackage{url}
\usepackage{blindtext}
\usepackage{enumitem}

\hoffset = -40pt
\voffset = -40pt
\textwidth = 500pt
\textheight = 760pt

\author{Floran NARENJI-SHESHKALANI \& Jean-Marcellin TRUONG} 

\begin{document}

Authors: Floran NARENJI-SHESHKALANI (643166) \& Jean-Marcellin TRUONG (643357)

\section{General idea}

The original design is composed of several IoT clients exchanging with a single
server on the same local area network (called IoT LAN further on).
The stated objective is to move that server to the cloud while keeping the
communications secure using a site-to-site VPN\@.
Beyond safety, one of the main design goals is to make this architecture
scalable in the sense that supporting more customers (more IoT LANs/server
pairs) should be straightforward.

From a technical standpoint, the servers in the cloud have their own local area
network (called cloud LAN further on) that is not connected to the internet.

\section{Proof of Concept}

This proof of concept will contain two customer networks: there will be two IoT
clients, two IoT gateways, one cloud gateway and two cloud servers.
The first customer is called `A' and the second one `B'.

\subsection{Docker}

While the scenario intends for this setup to be used in a corporate setting, the
environment has for obvious reasons to be simulated.
For that purpose, Docker is used. Docker offers a quick and easy way to setup
and script multiple simulated machines.
The main advantage of Docker over traditional virtual machines is ease of
reproducibility on different setups, which is key when doing groupwork.

Moreover, Docker is commonly used for cloud-based setups meaning that, for the
cloud part, our simulation is somewhat similar to a real-world application.
\\

Docker Compose is used on top of Docker. Docker Compose is a tool for managing
container instances: containers and networks are programmatically declared and
the infrastructure as a whole can then be managed using simple commands.

All instances and configuration of the instance can be found in the
\texttt{docker-compose.yml} file.
The images ran by the containers are situated in subfolders each containing a
\texttt{Dockerfile}.

\subsection{IoT device \& server}

The specifications state the limitations of the IoT device but nothing about
it's actual function.
Therefore, the simplest solution was chosen: a simple bash script runs inside
its own Docker container and simulates the IoT client.
This script uses curl to send a plain-text HTTP GET request containing the
current timestamp to a specified IP address/port.

The server is a simple stateless python script running inside its
own container.
The server receives the data from the client over HTTP, prints it out and
discards it.

Note that, due to a Docker technical limitation (host machine is seen as the
default gateway), the default route of both client and server is manually
overwritten to the correct gateway (the local VPN server).
\\

The IoT device image can be found in the \texttt{iot} subfolder.
The server image can be found in the \texttt{server} subfolder.

\subsection{Gateways}

The IoT gateways (one per customer network) each have their own tunnel to the
cloud gateway, which is common to all customers.
The gateways serve as the main routing nodes of their LAN network, and all
traffic goes through them.

They are also set up to only let through traffic that is relevant for each
network (see Section~\ref{firewall} for the firewall rules).
\\

The image for the client gateways is in the \texttt{gw} subfolder.
For each instance of this image, there exists a subfolder (\texttt{agw/} and
\texttt{bgw/}) which contains the certificates and key for the instance.
Through Docker Compose, this folder is mounted in the instance.

The cloud gateway image is in the \texttt{cloudgw/} subfolder.

\subsection{Usage}

To run the infrastructure, \texttt{docker} and \texttt{docker-compose} must be
installed.
Inside the folder, the following commands can then be used:\\
\texttt{%
    \# Build the images \\
    \$ docker-compose build
    \\ \\
    %
    \# Launch the gateways in the background \\
    \$ docker-compose up -d cloudgw agw bgw
    \\ \\
    %
    \# Launch the servers \\
    \$ docker-compose up asrv bsrv
    \\ \\
    %
    \# In another terminal, launch the client \\
    \$ docker-compose up aiot \\
    \$ docker-compose up biot \\
    \# Each client will make a request to its server and to the other server \\
    \# The one to the other server should fail.
    \\ \\
    %
    \# To shell into a gateway, e.g.\ the cloud one \\
    \# Note that exiting the shell will not stop the container \\
    \$ docker-compose exec cloudgw zsh
    \\ \\
    %
    \# Inside the container, you can use the following commands to observe the \\
    \# status of the tunnels and the firewall rules \\
    \$ > ipsec statusall \\
    \$ > iptables-save
    %
    \# To shut down the infrastructure \\
    \$ docker-compose kill
    \\ \\
    %
    \# To generate a new PKI (overwrites the previous, have to rebuild after) \\
    \$~./genpki.sh
}

\section{Technical choices}

\subsection{VPN}

Various options were considered for the VPN software, but it finally boiled down
to picking between OpenVPN and Strongswan.
Both pieces of software are well-suited to the usecases and are equally
strong and safe.

First of all, we already have experience in setting up OpenVPN on our own
dedicated servers.
Then, Strongswan implements IPSec, an industry standard that is quite widespread
on common routing infrastructure.
A client who would like to use its own router could readily configure
it so that it connects to our VPN (such configuration is outside the
scope of this project but is quite standard), which is a good selling point for
the customer and a decrease in cost for the company.

Thus, Strongswan was chosen as it was both a sound choice and a good learning
experience.

\subsubsection{Authentication (initial design)}
\textbf{%
\huge \danger{} \normalsize
This part talks only about our initial design from the first design document.
The final design is detailed in the next part.
}
\\

For authentication, a public key infrastructure with a root certificate and one
certificate for each of our gateways was the initial choice
Unfortunately, we had not yet managed to get it working by the initial design
deadline and authentication systematically failed.

Consequently, Pre-Shared Key (PSK) authentication was used, in which the
password is stored in clear-text in Strongswan's configuration file.
At the time, because of the scenario's simplicity (only two gateways were
simulated), we believed this to be as easily maintained as a
PKI-based authentication solution (more on that later).\\

In terms of security, we believe both solutions to be equivalent in most
regards: if the machine is compromised, then a PSK or a private key can be
extracted in the same manner, and both can then be replaced easily by an admin
after the breach. 
If the private key is password protected and the password is not provided in the
configuration files, Strongswan will ask for it on startup.
The same also applies to a PSK if so configured.

For an established connection, having access to the PSK is not a security
concern as it is only used for initial authentication and not as a session key
(the configuration used here provides perfect forward secrecy).\\

In the end, however, a PKI is vastly superior for multiple reasons.
First of all, an existing organization is likely to have its own PKI already,
and it would make sense to leverage such an existing system.

Then, while the current needs are quite simple and are easily satisfied by a
PSK, future needs might not be: there might be more sites, more clients and
potentially a need for revocation.
Such an evolution in requirements is clearly a usecase in favor the PKI\@: it
provides a tremendous advantage in scalability, maintenance and security.

Additionally, in a configuration with multiple customers, a successful attack
against the cloud gateway entails a full replacement of all PSKs, whereas it
would suffice to provide a new certificate and key pair when using a PKI\@.
Even safer, a smartcard can be used to hold the private key, in which case it
should be safe against both physical and remote attacks.

Finally, a key provided through a well configured PKI will always be
cryptographically safe, whereas a PSK chosen by sloppy students might only be 7
characters long (when it should have been a very long string of maximum
entropy).

\subsubsection{Authentication (final)}

For the final design, the public-key infrastructure is up and running.
The keys are generated outside of any Docker container (e.g.\ this is the
simulated equivalent of having an offline machine where the private key for the
root certificate is stored).

The gateway containers are preloaded with the certificates and key: there's no
runtime mechanism for obtaining them dynamically through an API or similar.
Indeed, having such an API is a safety risk (bigger attack surface) and lays
mainly outside of our domain of competence (we believe such difficult systems to
be best left to more qualified people, such as Verisign or Let's encrypt).

Each IOT gateway has its own key and certificate, the root certificate and the
cloud gateway's certificate.
The cloud gateway has its own key and certificate, the root certificate and all
certificates from the other gateways.
Note that checking only the name of the other party's certificate would also
have been a possibility, but checking both name and certificate offers
additional safety (e.g.\ if the root certificate's private key is compromised
but not the individual private keys, then the certificates should remain safe to
use until replacement).\\

Currently, each gateway stores its private key locally, which is a risk in case
of a breach.
However, Strongswan supports smartcards.
For a real-world application, it is the highest recommendation that smartcards
be used.

A smartcard is likely one of the safest ways to get a private key
to the customer's physical site: the smartcards are believed to be physically
tamperproof and would not fall victim to a remote attack.
One could even consider having a smartcard delivered to the client and
configuring the gateway remotely, implying much less time spent and therefore
reduced cost per client.

\subsubsection{Encryption algorithms}

Strongswan accepts 3 paramaters related to communications security.

For encryption, AES256-CBC is used, as it is a well-known standard and provides
included MAC\@.

For integrity, prfsha512, a sha512 hash combined with a pseudorandom
function, is used.

Finally, an elliptic curve Diffie-Hellman with curve25519 is used to
construct a session key.
This provides perfect forward secrecy.

\subsection{Routing}

For each customer network, a separate Docker container runs the back-end server.
Such a setup allows efficient use of the resources of the cloud servers: a
Docker container has a very small resource footprint and the application is
presumed not to be computationally heavy.

It also avoids systematic use of a reverse proxy: because each container has its
own IP in the cloud LAN, the IoT devices can contact its target directly.

From a security standpoint, we can ensure using the firewall on the cloud
gateway that communications are only flowing between devices belonging to
the same customer (cf Section~\ref{firewall}).
\\

Note that, should the load become too important for a single server, there are
multiple server-side options (ignoring software optimizations, sorted by order of
preference):
\begin{itemize}
    \item the container could be moved to its own cloud server (instead of
    sharing with other containers)
    \item the container could be moved to a more powerful cloud server
    \item a load-balancer could be added in front of a cluster of containers
        running on different machines
\end{itemize}

\subsection{Firewall}\label{firewall}

On the cloud gateway, the default iptables policy is to forbid all traffic.
Packets used by the IPSec protocol itself are then allowed on the
internet-facing interface only.

The key rules, of which there is a pair for each IoT network, are the following:
\\

\texttt{%
    -A FORWARD -p tcp -s \$LAN\_SOURCE/16 -d \$DEST\_SERVER/32 --dport 80 -i
    eth\_internet -o eth\_lan -m conntrack --ctstate NEW,ESTABLISHED -m policy
    --dir in --pol ipsec --reqid tunnel\_id --proto esp -j ACCEPT
}
\\

\texttt{%
    -A FORWARD -p tcp -s \$DEST\_SERVER/32 -d \$LAN\_SOURCE/16 --sport 80 -m
    conntrack --ctstate \\ESTABLISHED -i eth\_lan -o eth\_internet -m policy --dir
    out --pol ipsec --reqid tunnel\_id --proto esp -j ACCEPT
}
\\

The first rule will allow incoming traffic from the IoT LAN and to the
destination server (and only that one server) on the TCP port 80 (HTTP) coming
in from the internet and going towards the cloud LAN if it has travelled
through the correct IPSEC VPN tunnel (reqid parameter).

The second rule is the same but in the other direction.

Furthermore, both rules have statefulness constraints: traffic can only be
initiated from the IoT LAN to the cloud LAN and traffic can only leave the cloud
LAN through an already established connection.
\\

On the LAN gateway, the rules simply check that traffic has proper origin and
destination network and that it is indeed IPSEC traffic.
As safety-related rules are already enforced on the cloud gateway, those rules
are much more lax.
Generally, as the IoT gateways are in the client's hand, they should not be
trusted which explains why the cloud gateway has bigger responsibilities.

Firewall rules in full can be found in \texttt{gw/fw} for the IoT gateways and
\texttt{cloudgw/fw} for the cloud gateway.

\end{document}

